{"/":{"title":"jneo8.com","content":"\n# About\n\nThis a jneo8, a software engineer who focus on cloud native/database/devops/backend topics.\n\n\n# Articles\n\n- [List of articles](/tags/articles)\n","lastmodified":"2023-02-02T10:24:04.111266479Z","tags":null},"/publish/ClickHouse-Index-202301071508":{"title":"ClickHouse Index-202301071508","content":"\n# ClickHouse Index-202301071508\n\n# Summary\n\nHow Clickhouse index work?\n\n# Notes\n\n介紹 Index 前, 先介紹一下 Clickhouse MergeTree 的儲存結構\n\n## Partition\n\n一個 Partition 可以有多個 parts. Clickhouse 每次 bulk insert 都會產生新的 part file, 相同 Partition 的 part 會在 insert 完後 10-15分鐘 merge, 合併一個 part file\n\nPartition Format 有兩種: `Wide`, `Compact`\n\n* `Wide`: Column 分開儲存, default.\n* `Compact`: 所有 Column 一起儲存, 適合頻率大, 小量的 insert.\n\n文件中有一段話講 partition key 的設計\n\n\u003e A merge only works for data parts that have the same value for the partitioning expression.\n\u003e\n\u003e This means you shouldn’t make overly granular partitions (more than about a thousand partitions). Otherwise, the SELECT query performs poorly because of an unreasonably large number of files in the file system and open file descriptors.\n\n白話就是 partition 在 table 內不應該過多, 會降低 select 效能. 自己的心得就是針對不同的 data source 選擇適合的 partition key 可以有效改善效能瓶頸\n\n## Primary Key\n\n跟一般 RMDB 不同的是, Clickhouse Primary Key 並沒有表示重複數據的意思.\n而是依照 Primary Key 排序. 由於 Clickhouse 內可以設定多個 Primary Key, 比如說 __(Area, UserID)__, 表示說 Part 內數據先依照 __Area__ 排序, __Area__ 內再依照 __UserID__ 排序.\n\n### Granules\n\nGranules 是 Clickhouse data 的最小單位, granules 包含一個數量的 rows. Clickhouse 會拿 granules 第一筆資料的 Primary Key 當 mark.\n\n* 針對每個 Part, 會有個 index file `.idx` 儲存這些 mark value.\n\n* 針對每個 Columns, 就算不是 Primary Key, 也會儲存這些 mark. `.mrk` (透過 mark 在 column file 找資料)\n\n\u003e * `.idx` 存這些 mark value, `.mrk` 則是存兩個偏移值 `.bin 的偏移值定位到 compressed block` \u0026 `compressed block 解壓後的文件偏移量`\n\n### Block\n\nBlock 是 Clickhouse Column 的壓縮單位, 包含多個 Granules, 至於是多少要看 [min_compress_block_size](https://clickhouse.tech/docs/en/operations/settings/settings/#min-compress-block-size) \u0026 [max_compress_block_size](https://clickhouse.tech/docs/en/operations/settings/settings/#max-compress-block-size)決定.\n\n* 每次寫入一個 Granules (default `index_granularity` 8192 rows), Clickhouse 會檢查 data size\n    * 如果 \u003c `min_compress_block_size` 會繼續拿取 data\n    * 如果 \u003e `min_compress_block_size` \u0026 \u003c `max_compress_block_size` 會壓縮並寫入磁碟\n    * 如果 \u003e `max_compress_block_size`, 會依照 `compress_block_size` 截斷並生成 block\n\n一個 `.bin (Date file)` 內包含多個 compress block\n\n\n結構圖如下:\n\n![](/img/clickhouse/clickhouse-mergetree-layout-within-a-single-part.jpg)\n\n---\n\n## Index\n\n所以 Select Index 如何發揮作用呢 ？\n\n* Create Table T ORDER BY A\n    * A 有 Primary Index, B 沒有\n\n```sql\nCREATE TABLE T(A Int, B Int) MergeTree order by A\n```\n\n* 這邊 insert 三次, 所以會有三個 part 產生\n\n```sql\ninsert into T values(1, 1)\ninsert into T values(2, 1)\ninsert into T values(1, 2)\n\nselect database, table, name, primary_key_bytes_in_memory, marks_bytes, rows  from system.parts where table = 'T'\n\n```\n\n```\n┌─database─┬─table─┬─name──────┬─primary_key_bytes_in_memory─┬─marks_bytes─┬─rows─┐\n│ default  │ T     │ all_1_1_0 │                           8 │          80 │    1 │\n│ default  │ T     │ all_2_2_0 │                           8 │          80 │    1 │\n│ default  │ T     │ all_3_3_0 │                           8 │          80 │    1 │\n└──────────┴───────┴───────────┴─────────────────────────────┴─────────────┴──────┘\n```\n\n* 開啟 Tracing\n\n```sql\nset send_logs_level = 'trace'\n```\n\n\n* 針對 B 做 Query\n\n```sql\nSELECT * FROM T where B \u003e 1\n```\n\n```\n... \u003cDebug\u003e executeQuery: (from 127.0.0.1:53134) select * from T where B \u003e 1\n... \u003cTrace\u003e ContextAccess (default): Access granted: SELECT(A, B) ON default.T\n\n// primary index 沒作用\n\n... \u003cDebug\u003e default.T (9eae30d1-c8fa-4630-af28-88484187ddbe) (SelectExecutor): Key condition: unknown\n... \u003cTrace\u003e default.T (9eae30d1-c8fa-4630-af28-88484187ddbe) (SelectExecutor): Not using primary index on part all_1_1_0\n... \u003cTrace\u003e default.T (9eae30d1-c8fa-4630-af28-88484187ddbe) (SelectExecutor): Not using primary index on part all_2_2_0\n... \u003cTrace\u003e default.T (9eae30d1-c8fa-4630-af28-88484187ddbe) (SelectExecutor): Not using primary index on part all_3_3_0\n\n// Clickhouse 掃過全部三個 Partition\n\n... \u003cDebug\u003e default.T (9eae30d1-c8fa-4630-af28-88484187ddbe) (SelectExecutor): Selected 3 parts by partition key, 3 parts by primary key, 3 marks by primary key, 3 marks to read from 3 ranges\n... \u003cTrace\u003e default.T (9eae30d1-c8fa-4630-af28-88484187ddbe) (SelectExecutor): Reading approx. 3 rows with 3 streams\n... \u003cTrace\u003e InterpreterSelectQuery: FetchColumns -\u003e Complete\n┌─A─┬─B─┐\n│ 1 │ 2 │\n└───┴───┘\n... \u003cInformation\u003e executeQuery: Read 3 rows, 24.00 B in 0.004743004 sec., 632 rows/sec., 4.94 KiB/sec.\n... \u003cDebug\u003e MemoryTracker: Peak memory usage (for query): 0.00 B.\n\n1 rows in set. Elapsed: 0.007 sec. \n\n```\n\n```sql\nSELECT * FROM T where A \u003e 1\n```\n\n```\n... \u003cDebug\u003e executeQuery: (from 127.0.0.1:53134) SELECT * from T where A \u003e 1\n... \u003cTrace\u003e ContextAccess (default): Access granted: SELECT(A, B) ON default.T\n\n// 找到 Primary key, 透過找 LEFT/RIGHT boundary 選擇 partition\n\n... \u003cDebug\u003e default.T (9eae30d1-c8fa-4630-af28-88484187ddbe) (SelectExecutor): Key condition: (column 0 in [2, +inf))\n... \u003cTrace\u003e default.T (9eae30d1-c8fa-4630-af28-88484187ddbe) (SelectExecutor): Running binary search on index range for part all_2_2_0 (2 marks)\n... \u003cTrace\u003e default.T (9eae30d1-c8fa-4630-af28-88484187ddbe) (SelectExecutor): Running binary search on index range for part all_1_1_0 (2 marks)\n... \u003cTrace\u003e default.T (9eae30d1-c8fa-4630-af28-88484187ddbe) (SelectExecutor): Found (LEFT) boundary mark: 1\n... \u003cTrace\u003e default.T (9eae30d1-c8fa-4630-af28-88484187ddbe) (SelectExecutor): Found (RIGHT) boundary mark: 2\n... \u003cTrace\u003e default.T (9eae30d1-c8fa-4630-af28-88484187ddbe) (SelectExecutor): Found (LEFT) boundary mark: 1\n... \u003cTrace\u003e default.T (9eae30d1-c8fa-4630-af28-88484187ddbe) (SelectExecutor): Found empty range in 1 steps\n... \u003cTrace\u003e default.T (9eae30d1-c8fa-4630-af28-88484187ddbe) (SelectExecutor): Found (RIGHT) boundary mark: 2\n... \u003cTrace\u003e default.T (9eae30d1-c8fa-4630-af28-88484187ddbe) (SelectExecutor): Found empty range in 1 steps\n... \u003cTrace\u003e default.T (9eae30d1-c8fa-4630-af28-88484187ddbe) (SelectExecutor): Running binary search on index range for part all_3_3_0 (2 marks)\n... \u003cTrace\u003e default.T (9eae30d1-c8fa-4630-af28-88484187ddbe) (SelectExecutor): Found (LEFT) boundary mark: 0\n... \u003cTrace\u003e default.T (9eae30d1-c8fa-4630-af28-88484187ddbe) (SelectExecutor): Found (RIGHT) boundary mark: 2\n... \u003cTrace\u003e default.T (9eae30d1-c8fa-4630-af28-88484187ddbe) (SelectExecutor): Found continuous range in 2 steps\n\n// 這邊顯示只選到 1 個 partition\n\n... \u003cDebug\u003e default.T (9eae30d1-c8fa-4630-af28-88484187ddbe) (SelectExecutor): Selected 3 parts by partition key, 1 parts by primary key, 1 marks by primary key, 1 marks to read from 1 ranges\n... \u003cTrace\u003e default.T (9eae30d1-c8fa-4630-af28-88484187ddbe) (SelectExecutor): Reading approx. 1 rows with 1 streams\n... \u003cTrace\u003e InterpreterSelectQuery: FetchColumns -\u003e Complete\n┌─A─┬─B─┐\n│ 2 │ 1 │\n└───┴───┘\n... \u003cInformation\u003e executeQuery: Read 1 rows, 8.00 B in 0.005239063 sec., 190 rows/sec., 1.49 KiB/sec.\n... \u003cDebug\u003e MemoryTracker: Peak memory usage (for query): 0.00 B.\n```\n\n### 所以整個過程是什麼 ?\n\n1. Partition Key -\u003e Partition\n2. Mark LEFT/RIGHT -\u003e 選 Partition\n3. Primary key 找符合條件的 marks\n4. mark -\u003e 要讀取的範圍\n\nClickhouse Primary Index 是 Sparse Index, 針對特定 row 的 query 基本上很慢 (看整段過程就知道了), 但對條件過濾就友善很多了.\n\n所以 Clickhouse Index 的概念是什麼？\n\n*如何跳過更多的資料, 找到需要掃過的範圍*\n\n跳過的 Block 多了, 解壓縮的 Block 就少了. 可以很大程度的減少 I/O.\n\n就像切蛋糕, 看能夠切幾刀找到需要的資料\n\n## Tips\n\n* Unique: 避免太獨特的 primary key, 效果會很差\n\n* Long data range: 反之太常見的 Primary key 就會失去效果, 需要掃過的 range 過長\n\n* Primary Key 順序: Order by (A, B, C) or (B, A, C). 因為是 Sparse Index, 如果順序錯了有機會多讀取很多資料. 慎重考慮 query 條件 \u0026\u0026 Primary Key 的設定\n\n* 事後加入 Data Skipping Index, 對已經存在的資料並不會馬上發生作用. 而是需要 Part merge process 跑完後才會生效\n\n* 預設的 `min_compress_block_size` \u0026 `max_compress_block_size` 已經優化過了, 沒事別動.\n\n---\n\n# References\n\nhttps://www.slideshare.net/Altinity/webinar-secrets-of-clickhouse-query-performance-by-robert-hodges-173379008\nhttps://stackoverflow.com/questions/60255863/how-understand-the-granularity-and-block-in-clickhouse\nhttps://github.com/ClickHouse/ClickHouse/issues/7473\nhttps://stackoverflow.com/questions/60142967/how-to-understand-part-and-partition-of-clickhouse\nhttps://www.wangfenjin.com/posts/clickhouse-od/\n","lastmodified":"2023-02-02T10:24:04.115266491Z","tags":null},"/publish/Golang-goroutines-202301071522":{"title":"Golang goroutines-202301071522","content":"\n# Golang goroutines-202301071522\n\nA note describe how goroutine works\n\n# Notes\n\n\n## Concurrency is not Parallelism\n\n- Parallelism is doing multiple things at the same time.\n- Concurrency is dealing with multiple things at once(does not need to be done at the same time) with some time schedule and parallelism is subset of this.\n\n## What are threads?\n\n## Linux, task, process, thread\n\n[[publish/Linux task, process, thread-202301071513]]\n\n#### What make threads slow?\n\n- Threads have a large stack size (≥ 1MB) therefore consume a lot of memory. So imagine creating 1000s of thread means you already need 1GB of memory. That is a lot!\n- Threads need to restore a lot of registers, some of which include AVX( Advanced vector extension), SSE (Streaming SIMD Ext.), Floating Point registers, Program Counter (PC), Stack Pointer (SP) which hurts the application performance.\n- Threads setup and teardown requires call to OS for resources (such as memory) which is slow. NOT GOOD!\n\n## What about Goroutines?\n\n Goroutines exist only in the virtual space of the Go runtime and not the OS, therefore the Go Runtime scheduler is needed to manage their life-cycles.\n\n- **All the OS sees is a single user level process requesting and running multiple threads**\n\n- **The goroutines itself are managed by the Go Runtime Scheduler**\n\n```mermaid\nflowchart TD\n\nsubgraph go-binary\ngo-program\ngo-runtime\n\ngo-program \u003c--Memory Allocation--\u003e go-runtime\ngo-program \u003c--Channel communication--\u003e go-runtime\ngo-program \u003c--Creation of goroutines--\u003e go-runtime\nend\n\nOS-Kernel\n\ngo-runtime --syscalls--\u003e OS-Kernel\ngo-runtime --Thread creation--\u003e OS-Kernel\n```\n\nGo Runtime maintains four C structs for this purpose:\n\n- *The G(Goroutines) Struct*\n    - Represents a single goroutine\n    - Contains the fields necessary to keep track of its stack and current status. \n    - It also contains references to the code that it is responsible.\n    \n- *The M(OSThread) Struct*\n    - Represents a OS thread.\n    - Contains points to fields such as the\n        - global queue of runnable goroutines\n        - the current running goroutine, its own cache and the reference to the scheduler.\n    \n- *The Sched(Scheduler) Struct*\n    - A single, global struct \n    - Keeps track of the\n        - different queues of goroutines\n        - M's\n        - Information that the scheduler needs in order to run, such as the Global Sched Lock.\n\n- *The P(Processor) Struct*\n    - Each P has a local Goroutine queue.\n    - Each M should be assigned to a P.\n    - P may have no Ms if it's blocked or in a system call.\n\nThere are two queues contain G Struct:\n-  *1 in the runnable queue where M's (threads) can find more work*\n- *1 is the free list of goroutines*\n\n**There is only one queue pertaining to M's (threads) that the scheduler maintains. And in order to modify these queues, the Global Sched Lock must be held.**\n\n```mermaid\nflowchart TD\n\n%% class\nclassDef os-scheduler fill:#808000,color:#000;\nclassDef cpu fill:#98AFC7,color:#000;\nclassDef thread fill:#667C26,color:#000;\nclassDef scheduler fill:#488AC7,color:#000;\nclassDef global-run-queue fill:#bbf,stroke:#f66,stroke-width:2px,color:#000,stroke-dasharray: 5 5;\nclassDef local-run-queue fill:#f9f,stroke:#333,stroke-width:4px,color:#000;\n\n\nOS-Scheduler:::os-scheduler\nLogical-CPU-1:::cpu\nLogical-CPU-2:::cpu\nOS-Thread-1:::thread\nOS-Thread-2:::thread\nOS-Thread-3:::thread\nGo-Scheduler:::scheduler\nGlobal-Run-Queue:::global-run-queue\nLocal-Run-Queue-1:::local-run-queue\nLocal-Run-Queue-2:::local-run-queue\n\n\nOS-Scheduler{OS-Scheduler}\nLogical-CPU-1(Logical-CPU-1)\nLogical-CPU-2(Logical-CPU-2)\nOS-Thread-1([OS-Thread-1: M])\nOS-Thread-2([OS-Thread-2: M])\nOS-Thread-3([OS-Thread-3: M])\n\nProcesser-1(Processer-1: P)\nProcesser-2(Processer-2: P)\n\nGo-Scheduler{Go-Scheduler: Sched}\nGlobal-Run-Queue(Global-Run-Queue)\nLocal-Run-Queue-1(Local-Run-Queue-1)\nLocal-Run-Queue-2(Local-Run-Queue-2)\n\n\nG1((G1: G))\nG2((G2: G))\nG3((G3: G))\nG4((G4: G))\nG5((G5: G))\nG6((G6: G))\nG7((G7: G))\nG8((G8: G))\nG9((G9: G))\nG10((G10: G))\nGX1((...))\nGX2((...))\nGX3((...))\n\nsubgraph kernel-space\nOS-Threads\nOS-Scheduler\nLogical-CPU-1\nLogical-CPU-2\nend\n\nsubgraph user-space\n    GOMAXPROCS\n    Local-Run-Queue-1\n    Local-Run-Queue-2\n    Go-Scheduler\n    Global-Run-Queue\nend\n\n\nsubgraph Global-Run-Queue\n    G7 --\u003e G8 --\u003e G9 --\u003e G10 --\u003e GX3\nend\n\nsubgraph Local-Run-Queue-1\n    G1 --\u003e G2 --\u003e G3 --\u003e GX1\nend\n\nsubgraph Local-Run-Queue-2\n    G4 --\u003e G5 --\u003e G6 --\u003e GX2\nend\n\nsubgraph GOMAXPROCS\n    Processer-1\n    Processer-2\nend\n\nsubgraph OS-Threads\n    OS-Thread-1\n    OS-Thread-2\n    OS-Thread-3\nend\n\nOS-Scheduler ----\u003e Logical-CPU-1\nOS-Scheduler ----\u003e Logical-CPU-2\nLogical-CPU-1 --\u003e OS-Thread-1\nLogical-CPU-2 --\u003e OS-Thread-2\nLogical-CPU-2 --\u003e OS-Thread-3\n\nGo-Scheduler ==\u003e Global-Run-Queue\nOS-Thread-1 -.-\u003e Processer-1 -.-\u003e Local-Run-Queue-1\nOS-Thread-2 -.-\u003e Processer-2 -.-\u003e Local-Run-Queue-2\nOS-Thread-3 -.-\u003e Processer-2\n\n```\n\n\n - **GOMAXPROCS**:\n    - $$Num(P) = GOMAXPROCS$$\n     - Go application the number of threads available for Goroutines to run is equal to the GOMAXPROCS,  which by default is equal to the number of cores available for that application.\n - Golang has an NumA:NumB scheduler that also utilizes multiple processors.\n     - At any time, NumA goroutines need to be scheduled on NumB OS threads that runs on at most GOMAXPROCS numbers of processors(NumB \u003c= GOMAXPROCS)\n\n### So here is the catch of Goroutines\n\n- A lightweight abstractions over threads\n    - The **memory consumption** is better than thread. The creation of goroutines require much lesser memory as compared to threads. It requires 2kb of memory, while threads requires 1MB.\n    - **Threads have significant setup and teardown costs** because it has to request resource from the OS and return it once it's done. While Goroutines are created and destroyed by go runtime.\n    - **Switch cost**:\n        * Threads are scheduled preemptively. If the process is running for more than a scheduler time slice, it would preempt the process and schedule execution of another runnable process on the same CPU, the scheduler needs to save/restore all registers.\n*  In Golang, unlike normal functions, the control does not wait for the Goroutine to finish executing. The Control immediately returns to the next line of the code after a Goroutine call\n\n---\n\n# References\n\n- [What are goroutines? And how do they actually work? | by João Henrique Machado Silva | The Polyglot Programmer | Medium](https://medium.com/the-polyglot-programmer/what-are-goroutines-and-how-do-they-actually-work-f2a734f6f991)\n- [A complete journey with Goroutines | by Riteek Srivastav | Medium](https://riteeksrivastava.medium.com/a-complete-journey-with-goroutines-8472630c7f5c)\n- [Coroutine - Wikipedia](https://en.wikipedia.org/wiki/Coroutine)\n- [How Goroutines Work](https://blog.nindalf.com/posts/how-goroutines-work/)\n","lastmodified":"2023-02-02T10:24:04.115266491Z","tags":null},"/publish/How-about-RSS-202301071503":{"title":"How about RSS?-202301071503","content":"\n# How about RSS?-202301071503\n\n# Summary\n\nHow RSS has changed the way I get information?\n\n# Notes\n\n## RSS\n\nhttps://en.wikipedia.org/wiki/RSS\n\nReally Simple Syndication\n\n是種利用 XML 特性的資料來源格式\n\n主要興起的時間是 blog 盛行的年代\n\n以 2020 來說, 身邊年紀相當或者比較年輕的朋友使用的人是微乎其微\n\n主因大概就是社群媒體的風行\n\n**那我為何選擇 RSS ?**\n\n---\n\n## 從被餵養資訊到主動選擇資訊\n\n今日幾個主要的社群軟體 FB, IG, Twitter\n\n貼文的雜訊很多\n\n內部還混雜著各種從小到大的同學, 朋友, 長輩, 同事\n\n它們並不是一個適合資訊閱讀的平台\n\n你會被各種新聞, 貼文, 動態, 廣告轟炸\n\n演算法給你的資訊大部份跟你的人生無關\n\n社群媒體的設計就是要讓你花大把時間在上面\n\n像是被圈養的羊, 羊吃草你吃垃圾資訊\n\n你並不能選擇對你真正有意義的資訊\n\n**我們要社交, 但不是過度社交資訊轟炸**\n\nRSS 是種資訊獲得的手段\n\n滑 RSS 跟 滑塗鴉牆是相同的動作\n\n你一樣滑手機, 一樣獲得資訊\n\n不同的是\n\n* 一個給你大量的演算法訊息\n\n* 一個是你要自己經營, 過濾, 識別內容的塗鴉牆\n\n就像看報紙一樣, 但你要自己決定誰可以進入你的版位\n\n## 選擇適合自己的資訊來源\n\n每個人喜歡或偏好的東西都不相同\n\n但好消息是主流的媒體大部份都是有RSS的\n\nGoogle 是很方便的工具, 你不會找不到適合你的\n\n但個人經驗是大約一兩個月還是要整理一次\n\n畢竟總要磨合才知道喜不喜歡\n\n## 我是一個 Engineer\n\n因為是 Engineer\n\n個人習慣追蹤大量的 Engineer Blog, Developer Blog, Weekly News, Release notes\n\n為此 RSS 其實是一個滿方便的工具\n\n方便追蹤一些 blog release, Weekly News\n\n我還滿篤信盡量追蹤第一手資訊(不會出現在社群網站) 會讓自己更可以貼近某一個開發社群\n\n就像如果你今天上班通勤30分鐘\n\n可能就已經看了一到兩篇文章\n\n而不是20篇網紅動態\n\n但相同類別看多也會膩\n\n好處是你可以幫它們分類\n\n想看就看, 不想看可以換成比較輕鬆的類別(電影, 新聞, 音樂類)\n\n長期就比較不會有被轟炸的感覺\n\n## 讓自己成為雜食性閱讀者\n\n個人是滿相信點線面的概念的\n\n一個貼文或者觀念是一個點\n\n相同領域的知識是一個面\n\n不同面向的知識匯集時它才會成為一個觀點\n\n當一個人多面向的知識被串起來時\n\n才可以從比較成熟的角度去看待某些事情\n\n\n## 選擇工具\n\nRSS Reader 的選擇真的很多\n\n目前是用 [feedly](https://feedly.com)\n\n簡單, 整潔, 有手機版本\n\n但工具不是重點就是了, 你也可以挑自己喜歡的\n\n---\n\n# Recap\n\n並不是說社群軟體一定很差\n\n自己忍不住也是會滑個兩下\n\nRSS 是古時代的產物了\n\n但就像有些人偏好實體書, 有些人不愛網路購物一樣\n\n社群軟體的進步換來的可能是一群長期被特定資訊餵養的群眾\n\n就像你看書會挑書的內容, 但你每天花大把時間的社群媒體內容(還是演算法給你的)卻全盤接受\n\n選擇優良的資訊來源也很重要喔!!\n\n---\n# References\n","lastmodified":"2023-02-02T10:24:04.115266491Z","tags":null},"/publish/Linux-task-process-thread-202301071513":{"title":"Linux task, process, thread-202301071513","content":"\n# Linux task, process, thread-202301071513\n\n# Summary\n\nDefinition of thread, process and task in Linux\n\n# Notes\n\n## Definition of process\n\nProcess Includes:\n\n- `Code Section`\n- `Data Section`\n- `OS Resources (e.g. files)`\n\n$$Code Section+Data Section = Memory$$\n$$Memory = (text segment + data segment + heap segment)$$\n\n### `PCB (Process Control Block)`\n\nA PCB is the way how operating system handle process. The PCB is a form created by OS for record the status of process. When switch to different process, the status will be keep in the PCB and recover from the latest status when switch back.\n\n### Process switch called `Context Switch`, when will Context Switch happen?\n\n- External Interrupt ( Emit by components exclude CPU: I/O interrupt、 Timers)\n- Internal Interrupt ( Emit by CPU: stack overflow、divided by zero …) \n- Software Interrupt (e.g. System Call: **user mode switch to kernel mode**)\n\n## Definition of thread(Light weight process)\n\nA minimum unit for CPU Scheduling, can not exist alone. Must exist in the process.\n\nA thread includes:\n\n- `Stack`\n- `Register Set`\n- `Program Counter`\n\nThe threads share below resource in a single process:\n\n- `Code Section`\n- `Data Section`\n- `OS Resources`\n\n```mermaid\nflowchart TD\n\nsubgraph Process\nCode-Section\nData-Section\nOS-Resources\n\nthread-A\nthread-B\nthread-C\nend\n\nsubgraph thread-A\nStack-A\nRegister-Set-A\nProgram-Counter-A\nend\n\nsubgraph thread-B\nStack-B\nRegister-Set-B\nProgram-Counter-B\nend\n\nsubgraph thread-C\nStack-C\nRegister-Set-C\nProgram-Counter-C\nend\n```\n\nA process at least has one thread\nA process can have multiple threads\n\n*Process is the object that the OS allocates resources*\n*Thread is the object that OS allocates CPU times*\n\n## Kernel-level and user-level thread\n\n*User level thread* is managed by the user-level library to handle `Thread Context Switch`, which only process in user-level so that kernel will never know it exists and never target `System Call`\n\n*Kernel level thread* is managed by OS. The `Thread Context Switch` costs lower that `Process context switch` because it doesn't need `address space`. Specialized for `System Call`\n\n## Inside Linux \n\nThe Linux doesn't have the clearly concept of process and thread; however; the concept inside Linux called `Task`. The `task_struct`, which is also be called `process descriptor`, is the data structure to implement Task.\n\n```c\n/* task_struct 記錄著目前這 Process 的狀態 */\n/* task_struct 就有一個指向 kernel stack 的變數 stack */\n/* task_struct 就有一個指向 mm_struct 的變數 mm_struct */\nstruct task_struct {\n  volatile long state; // -1 unrunnable, 0 runnable, \u003e0 stopped\n  void *stack; // point to kernel stack\n  struct mm_struct *mm // point to memory descriptor\n};\n```\n\nIf two task share one `mm_struct`, they are called `Thead`\n\n```mermaid\nflowchart TD\n\ntask_struct_A --mm--\u003e mm_struct\ntask_struct_B --mm--\u003e mm_struct\n```\n\nmm_struct is also called `Memory descriptor` which describes the virtual address space for this process.\n\n```c\n/* mm_struct 有一個 Process(main thread) 的 user stack 起始位置 */\nstruct mm_struct {\n  unsigned long start_stack // user stack start address\n};\n```\n\nInside `mm_struct`, there is a `start_stack`, which is the start address of `user stack`, provides the store space for the processing in the User mode. If it is a Thread, because parent share the `mm_struct`, it will found another space in `heap` or `mmap` for Thread to avoid affect the stack in `mm_struct`.\n\nNo matter how to create `user stack`, it will initiate **task_struct-\u003ethread-\u003esp** to store the user stack's message.\n\n```c\n/* task_struct 就有一個 thread_struct 的變數 */\nstruct task_struct {\n  struct thread_struct thread; // 存放 context switch 相關的資訊\n};\n\n/* thread_struct 保留了大部分的 CPU registers 的訊息，context switch 主要就是回復保存在 thread_struct 裡的資訊 */\nstruct thread_struct {\n  unsigned long  sp0; // 存放 kernel stack base address  \n  unsigned long  sp;  // 存放 kernel stack (current)top address\n};\n```\n\nSo if we create a process/thread (task_struct), we will get two stacks:\n\n- `user stack`:\n    - Build on address space(mm_struct), to use process(mm_struct's stack) or Thread(mm_struct's heap/mmap) in user mode.\n- `kernel stack`:\n    - Build on kernel address space, use task_struct point to stack(thread_info) in kernel mode.\n\n\u003e `kernel address space`: The memory space share between all  kernel stack\n\u003e\n\u003e `thread_info`: Inside struct thread_union and relate to context switch\n\n\n\n---\n# References\n\n[OS Process \u0026 Thread (user/kernel) 筆記 | by Yovan | Medium](https://medium.com/@yovan/os-process-thread-user-kernel-%E7%AD%86%E8%A8%98-aa6e04d35002)\n","lastmodified":"2023-02-02T10:24:04.115266491Z","tags":null},"/publish/golang-project-architecture-202301071505":{"title":"golang project architecture-202301071505","content":"\n# golang project architecture-202301071505\n\n# Summary\n\n結構化的 Golang project 設計\n\n# Notes\n\n這是個老議題了, 但最近在網路上看到滿多教學文在講 project setup\n\n還是來分享一下自己目前的理解跟設計\n\n## 專案架構\n\n```bash\n.\n├── assets\n├── bin\n├── build\n├── CHANGELOG.md\n├── cmd\n│   ├── demo1\n│   │   └── main.go\n│   └── demo2\n│       └── main.go\n├── configs\n├── deployment\n├── docs\n├── entity\n├── go.mod\n├── go.sum\n├── infra\n├── Makefile\n├── pkg\n├── README.md\n├── repository\n├── scripts\n└── usecase\n\n```\n\n## Entity Layer\n\n### /entity\n\n* 在說明 entity 放什麼之前, 先看一下 entity 的 解釋\n\n    \u003e __Entities encapsulate Enterprise wide business rules. An entity can be an object with methods, or it can be a set of data structures and functions. It doesn’t matter so long as the entities could be used by many different applications in the enterprise. If you don’t have an enterprise, and are just writing a single application, then these entities are the business objects of the application. They encapsulate the most general and high-level rules. They are the least likely to change when something external changes. For example, you would not expect these objects to be affected by a change to page navigation, or security. No operational change to any particular application should affect the entity layer.__\n\n* 所以 entity 是什麼 ?\n    * 是最高級的實體, 包含一些針對實體的 function\n    * 變動最小 (你不會希望你到處引用的 struct 一直被修改)\n    * 因為是最高層的, 不會引用別人, 會被各個 application 引用\n    * entity 並不包含 database/service/application, 是最簡單的封裝\n\n## Drivers\n\n### /repository\n\n* Repository Pattern\n\n* 通常都是定義好對應的 interface 再用某個 Datebase 去實做, 好處是可以隔離 db \u0026 service. 例如 usecase 使用 repository, 對他來說就看不見下層的 Database.\n\n* 個人覺得有個麻煩的地方就是雖然有辦法抽換 Database, 但如果初期沒有規劃好 interface 的話, 有可能會變成看似隔開了但其實沒有. 比較好的訣竅可能是在設計 Repository 時不應該針對特定 Database. 自己是覺得這方面自己做的不太好就是了.\n    * 另外就是抽換真的很麻煩, 需要實做 interface, 評估效能..etc.\n\n## Usecase Layer\n\n### /usecase\n\n* 基本上業務邏輯會被集中在這層, 通常是放 Service\n* 改動程度最大\n\n## Interface Adopters Layer\n\n### /cmd\n\n* Main application package\n* 相較有些 project 會把 main.go 放在根目錄, 如果你的 application 需要多個 binary(只有一個也沒關係), 個人認為 cmd 是更好的選擇.\n* 這邊通常不會有太多 business logic, 主要都只有CLI, 讀取設定, Dependency injection 和 service 啟動的簡單邏輯\n* 編譯完的 binary 都會放到 /bin\n\n### /api\n\n* API Documents OpenAPI/Swagger\n* Router / Middleware / httputils\n* Examples\n    * https://github.com/moby/moby/tree/master/api\n\n## Support packages\n\n### /pkg\n\n* logging/file handling/encryption, etc.\n* 其它 package(api/usecase/cmd)可以引用\n* 不是核心業務邏輯\n    * 這部份私人認為判斷上有其難度, 如何切分業務邏輯, 這也是每個 Engineer 要下的決定\n\n## Common projects directories\n\n### /config\n\n* 設定用的 yaml\n\n### /docs\n\n* 嗯, 就是文件\n\n### /infra\n\n* 快速的 Infrastructure setup\n* 有些教學會把 repository 放進來, 但個人認為一個完整的 project 可能會需要設定開發or測試時所需要的 infrastcture, 比如說某個Database, 這邊主要放這些 setup 的 tools/scripts.\n\n### /assets\n\n* 靜態文件\n\n### /bin\n\n* 放編譯完的執行檔\n\n### /Scripts\n\n* Shell script files\n* 避免 Makefile 過於肥大, 太過複雜的 script 就放這\n\n### /deployments or /deploy\n\n* 主要是部署文件\n* docker-compose\n* kubernetes/helm\n* mesos\n* terraform\n\n### /build\n\n* Build/CI scripts and config\n* CI config \u0026 scripts -\u003e `build/ci`\n    * 有些 CI 會指定檔案位置, 如果可行的話儘量擺這 \n\n### /test\n\n* Cross-functional test suites\n\n# Recap\n\n這是大多數在工作上常用的配置, 好處是因為有依照 clean architecture 去切分 package, 大多數的改動只需要動到單一幾個package.\n會讓 PM 產生一種你 code 寫很快的錯覺.\n另外要廢言一下怎麼評估自己的 project 切分的好不好\n\n* 對新人的易上手程度\n    * 設計上就是簡單易懂就好\n    * 文件 +　Script 的完整度\n    * 變相降低 switch 成本\n* Business rule 切分\n    * 這滿吃經驗的, 自己也是學習中\n* Dependency Injection\n    * DI 做的好, 測試沒煩惱\n    * 抽象程度的拿捏也需要經驗跟練習\n\n最後要說, 大多數時間 project 都不是完美的架構, 可能需要經過時間跟專案複雜度微調\n就像一些實驗性的 project 可能就不會這樣完整\n架構也是慢慢發展起來\n\n---\n\n# References\n\n* https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html\n* https://eltonminetto.net/en/post/2020-07-06-clean-architecture-2years-later/\n* https://github.com/golang-standards/project-layout\n","lastmodified":"2023-02-02T10:24:04.115266491Z","tags":null},"/publish/kubernetes-create-deployment-api-call-flow-202302021820":{"title":"kubernetes create deployment api call flow-202302021820","content":"\n# kubernetes create deployment api call flow-202302021820\n\n# Summary\n\nA diagram describes the flow create a simple deployment in kubernetes\n\n# Notes\n\n![[kubenetes-api-call-flow.png]]\n\n---\n# References\n","lastmodified":"2023-02-02T10:24:04.115266491Z","tags":null},"/publish/mermaid-golang-DI-and-CLI-builder-202301071458":{"title":"mermaid - golang DI and CLI builder-202301071458","content":"\n# mermaid - golang DI and CLI builder-202301071458\n\n# Summary\n\nMermaid is a tool helping user to use dependency injection more easily. By using dig, cobra and viper together.\n\n# Notes\n\n## Project\n\nlink: **[mermaid](https://github.com/jneo8/mermaid)**\n\nMermaid 這個專案的誕生, 是因為自己工作上時常需要寫cli tool而誕生的一個golang專案\n\n介紹Mermaid之前, 可能需要先介紹一些使用場景:\n\n- 希望透過 `-h`, `--username` 類似的指令把參數帶給entrypoint\n\n- 有讀取環境參數或者設定檔的需求\n\n- Logger\n\n- Dependency Injection\n\n- Testing(這很重要)\n\n如果上述需求你有符合, 那你就可以用Mermaid來開發你的程式\n下面來說說這個專案的一些想法以及開發過程\n最後再說使用方式\n\n---\n\n首先要達到上述的需求會有一些困難需要克服\n\n## Dependency Injection\n\n- DI是可以手動完成的\n- 如果你有很多重複性高的指令, 手動注入依舊可行, 但免不了需要寫大量的code, 以及來回驗證參數沒有打錯來確保正確性跟一致性\n- 生為一個懶惰的工程師, 直接放棄手動注入(是說一開始我也是手動做的, 手真的會酸)\n- 所以 DI 最後選擇透過[dig](https://github.com/uber-go/dig) 這個libary來完成\n- 因為使用了DI Libary, 所以如果有參數漏傳或者設定錯誤是會直接噴error的, 間接免去了檢查的時間\n\n## Config\n\n談config 之前, 我們可以先想看看有哪些東西是我們需要處理的範圍\n\n- `Default variable`: Default var, 會寫在程式內\n\n- `Config file`: Config file, e.g., yaml.\n\n- `environment variable`: Get from environment.\n\n- `Input arguments`: From cli args input.\n\n一般來說要符合使用, 引用順序應該會是\n\n**`default` -\u003e `config` -\u003e `environment` -\u003e `input args`**\n\n- 後段的會覆蓋前面的. 這樣的順序應該不論是cli或者container都適用\n\n最後lib 選擇\n\n- `config` \u0026 `environment variable`使用的是[viper](https://github.com/spf13/viper)\n\n- `default variable` \u0026 `input args` 使用的是 [cobra](https://github.com/spf13/cobra)\n\n而我們為了把所有的設定都給DI, 決定把他變成一個唯一的entity -\u003e [viper.Viper](https://godoc.org/github.com/spf13/viper#Viper). 所以實作上\n\n- `default`, `input args` -\u003e **cobra**\n- `config`, `environment` -\u003e **viper**\n- **cobra** -\u003e **viper**\n\n後續我們只需要處理**viper**跟**dig**之間的綁定就好\n\n## Logger\n\n為什麼會把 Logger 抽出來說, 是因為我認為一個好的cli tool, 他的Logger需要有一些特點:\n\n- 獨立的entity\n\n    - 大部份的logger lib, 都有一個global的entity\n        - 這件事會讓你的測試比較困難, global的東西互相影響不好測試\n        - 但如果你每次寫新的command, 都要new一次, 也是滿煩的\n\n    - 不同的component, 不同的logger. 這件事就是看開發或者業務需要\n\n- Customized\n\n    - 算必須. 大部份的lib也都有\n\nLogger lib 是用 [logrus](https://github.com/sirupsen/logrus)\n\n\n## Testing\n\nglobal的東西真的不好測試, 所以為了讓東西變得測試容易\n\n- **cobra** 只負責抓default跟args, 還有當進入點. \n\n- **viper** 基本上runtime才會init, command 之間不會影響\n\n- Logger同上\n\n這樣的意思是說, 每個cmd之間, config跟logger都是獨立的, 這會讓測試變得容易許多\n\n\n---\n\n# Recap\n\n嗯, 說了一些實作上的想法.\nProject本身沒什麼難度, 但其實省去了個人很多的開發時間, 希望大家可以使用看看\n\n最後再附上一次連結\n[mermaid](https://github.com/jneo8/mermaid)\n\n---\n# References\n","lastmodified":"2023-02-02T10:24:04.115266491Z","tags":null},"/publish/poromodo-202301071453":{"title":"poromodo-202301071453","content":"\n# poromodo-202301071453\n\n# Summary\n\n番茄鐘工作法摘要\n\n# Notes\n\n## 如何實行\n\n\n1. 決定待完成的工作\n2. 設定番茄鐘 ( 25 mins )\n3. 持續工作一個番茄鐘 `X`\n    * 前五分鐘計畫\n    * 後五分鐘 review\n4. 休息五分鐘\n5. 每四個 `X` 休息 15 - 30 分鐘\n\n\n## 原則\n\n* 番茄鐘不可分割\n\n* 時間表優先於番茄鐘\n\n\n## 表格\n\n* 事項盤點表\n    * `U` (unplanned) 給不緊急事項, 註明截止期限\n\n* 今日工作表\n    * 上半部 今日待辦事項\n        * 紀錄待辦事項跟番茄鐘\n    * 下半部 緊急處理事項\n        * 緊急事項寫在這\n\n* 番茄鐘\n    * 內部干擾 註記 `'`\n    * 外部干擾 註記 `-`\n\n---\n\n# References\n","lastmodified":"2023-02-02T10:24:04.115266491Z","tags":null},"/publish/python-asyncio-202301071423":{"title":"python asyncio-202301071423","content":"\n# python asyncio-202301071423\n\n# Summary\n\n在這篇文章你會學到:\n\n- AsyncIO 基本概念\n- Awaitable: Coroutines, Tasks and Future\n- AsyncIO Context Manager \u0026 Iterator\n\n# Notes\n\n## What is AsyncIO ?\n\n看一下官方 Document 的解釋\n\n\u003e [**asyncio is a library to write concurrent code using the async/await syntax.**](https://docs.python.org/3.9/library/asyncio.html)\n\n常見的誤區是許多剛接觸的人沒有辦法分辨出 **multithreading vs mulitprocessing vs asyncio**, 但這篇並不會詳細分析什麼情況要使用那一種 concurrency, 為了方便還是給一下基本原則:\n\n```python\nif io_bound:\n    if io_very_slow:\n        print(\"Use Asyncio\")\n    else:\n       print(\"Use Threads\")\nelse:\n    print(\"Multi Processing\")\n```\n\n那什麼是 AsyncIO ?\n\n當CPU 執行速度比 IO 快, 為了節省 CPU 等待的浪費, 所以在 CPU 等待 IO 的過程中讓 CPU 去執行其它任務\n\n個人最喜歡的比喻是像 [The Queen's Gambit](https://en.wikipedia.org/wiki/The_Queen%27s_Gambit_(miniseries)) 中有一段女主角一個人跟整個西洋棋社下棋的片段\n\n主角就是運作速度極快的 CPU, 其它人則是速度比較慢的 IO. 主角並不會等待 A 下完才換跟 B 下棋. 而是一但有人移動棋子了, 她就會切換過去給出她的下一步\n\n這樣因為 CPU 跟 IO 不等速, 節省時間而切換 CPU 的狀況就是 AsyncIO\n\n先來張基本流程示意圖\n\n```mermaid\ngraph LR\n    style handler fill:#bbf\n    style EventLoop fill:#f9f\n    style IO fill:#abc\n\n    handler --\u003e EventLoop((Event Loop))\n    EventLoop -- trigger callback ---\u003e handler\n    EventLoop -- Register Callback ---\u003e IO((IO))\n    IO -- Operation Complete ---\u003e EventLoop\n```\n\n想要了解 python asyncio, 就需要理解幾個相關詞跟概念\n\n* **Event loop**\n* **Callback**\n* **Coroutines**\n* **Tasks**\n* **Futures**\n\n那我們開始吧!\n\n## Event Loop\n\n\u003e 西洋棋的比喻: 監聽中的對手名單\n\n負責跑 asynchronous tasks \u0026\u0026 callbacks\n\nEvent loop 會跑在 單一 thread (main thread) 中, 就像一個 todo list \n\nmain thread 會監聽這個 TODO List, 等待它們完成後 呼叫 Callback\n\n## Callback\n\n\u003e 西洋棋的比喻: 對手(任務)\n\n\u003e A subroutine function which is passed as an argument to be executed at some point in the future.\n\n關於 callback 其實在 JS 中很常見, python 中的 callback function:\n\n```python\n\ndef func_b():\n    print(\"func_b\")\n\ndef func_a(callback, *args, **kwargs):\n    print(\"func_a\")\n    callback()  # exec callback func\n\nfunc_a(func_b)  # func_b is callback func_b for func_a\n#  func_a\n#  func_b\n```\n\n在 asyncio 中, callback 是透過 event loop schedule function 被註冊到 EventLoop\n\nEventLoop 的 schedule functions\n\n* [Scheduling callbacks](https://docs.python.org/3/library/asyncio-eventloop.html#scheduling-callbacks)\n* [Scheduling delayed callbacks](https://docs.python.org/3/library/asyncio-eventloop.html#scheduling-delayed-callbacks)\n\n## Awaitables\n\n\u003e Future-like object or a coroutine object\n\nPython 有三個主要的 awaitable objects.\n\n* **Coroutines**\n* **Tasks**\n* **Futures** .\n\n### [Coroutines](https://docs.python.org/3.9/library/asyncio-task.html#coroutines)\n\nCoroutine function: 可以中途 enter/exit/resumed 的 function\n\n感覺跟 callback 很像對吧?\n\n因為 callback 是 low-level API 中的概念(在 EventLoop 中實做). 而 Coroutines 是 high-level API. 透過 `asyncio.run` 執行\n\n```python\nimport asyncio\n\n# nested 是一個 Coroutine\nasync def nested():\n    return 42\n\nasync def main():\n    nested()  # 因為沒有 await, 所以 nested 並不會執行\n\n    print(await nested())  # 這邊會印出 42\n\nasyncio.run(main())\n```\n\nPython Coroutine 在 3.4 asyncio 出現之後從原本的 **Generator-based coroutines**,\n多加了 **Native coroutines**\n\n而其實 Coroutine 這個詞彙在 python 可以同時表示:\n\n* A coroutine function： `async def` function;\n* A coroutine object: an object returned by calling a coroutine function.\n\n所以當有人說 coroutine, 在 python 內可能是\n\n* **Native coroutines** / **Generator-based coroutines**\n* **Coroutine function** / **Coroutine object**\n\n**怎麼區分 Native coroutines \u0026\u0026 Generator-based coroutines 呢?**\n\n1. Native coroutine 沒有 `__iter__` and `__next__` methods.\n    - 不能 iterated over \n    - 不能傳給 `iter()`, `list()`, `tuple()` ...等等內建語法\n    - 不支援原本的 `for ... in loop` 語法\n\n2. Generators-based coroutines 可不可以使用 `yield from` ?\n    - 不能對 Native Coroutine 使用 `yield from` -\u003e `TypeError`\n    - 透過 [@asyncio.coroutine](https://docs.python.org/3/library/asyncio-task.html#asyncio.coroutine) decorator 包裝過的 generator 對 Native coroutine 使用 `yield from`\n\n3. inspect\n    - `inspect.isgenerator()` \u0026\u0026 `inspect.isgeneratorfunction()`\n        - Native coroutines return False\n    - 詳細的 inspect 可以看 [doc](https://docs.python.org/3/library/inspect.html)\n\n### Tasks\n\n* Task 設計是為了讓 coroutines 並發執行, 調度使用的\n\n* 一個 Event loop 同時只會跑一個 task\n\n```python\nimport asyncio\n\nasync def say_after(delay, what):\n    await asyncio.sleep(delay)\n    print(what)\n\nasync def main():\n\n    print(\"start\")\n\n    # 將 task1 \u0026\u0026 task2 註冊到 event loop 中\n    task1 = asyncio.create_task(say_after(1, \"hello\"))\n    task1 = asyncio.create_task(say_after(2, \"world\"))\n\n    # 等到 兩個 task 都執行完, 大約需要兩秒(不是三秒)\n    await task1\n    await task2\n\nasync.run(main())\n```\n\n### [Future](https://docs.python.org/3.9/library/asyncio-future.html#asyncio.Future)\n\n\u003e Future objects are used to bridge low-level callback-based code with high-level async\n\n* asynchronous operation 回傳的最終結果\n* Futures 是 low-level 的 awaitable object\n* 通常 application level 不會需要操作到 future\n\n* Task \u0026\u0026 Coroutines 都是 **future-like** object\n    \u003e future-like object\n    \u003e An object with an __await__ method, or a C object with tp_as_async-\u003eam_await function, returning an iterator. Can be consumed by an await expression in a coroutine. A coroutine waiting for a Future-like object is suspended until the Future-like object's __await__ completes, and returns the result. See Await Expression for details.\n\n\n## Context Manager \u0026\u0026 Iterator\n\n### Asynchronous Context Manager\n\n`async with`\n\n基本的 python context manager 需要定義兩個 funcs, `__enter__` \u0026\u0026 `__exit__`.\n\npython asynchronous context manager 則是需要定義兩個 magic method: `__aenter__` \u0026\u0026 `__aexit__`\n\n```python\nclass AsyncContextManager:\n    async def __aenter__(self):\n        await log('entering context')\n    async def __aexit__(self, exc_type, exc, tb):\n        await log('exiting context')\n```\n\n\n### Asynchronous Iterator\n\n`async for`\n\n基本 Iterator 需要定義 `__iter__`, `__next__`\n\nAsync Iterator -\u003e `__aiter__`, `__anext__`\n\n```python\nclass AsyncIterable\n    def __aiter__(self):\n        return self\n    async def __anext__(self):\n        data = await self.fetch_data()\n        if data:\n            return data\n        else:\n            raise StopAsyncIteration\n```\n\n---\n\n# Recap\n\n如果是第一次接觸 python 的 asyncio 的人, 有可能會搞混 Tasks, \nFuture, Coroutines 這些詞彙的意思.\n\n老實說第一次接觸時也是一頭混亂. 寫這篇時也是不斷的翻 PEP 的內容.\n\nAsyncio 的 API 變動其實滿快的. 所以詳細還是要看 doc\n\n一個成熟的工程師除了使用工具解決問題, 其實還要了解使用的時機跟內部的實作去避免踩雷跟解釋出自己是如何解決問題的\n\n想要研究(自虐) 的人還是把 PEP 看完會有更深的了解喔\n\n---\n\n# References\n\n## PEP List\n\n[PEP 479 -- Change StopIteration handling inside generators](https://www.python.org/dev/peps/pep-0479/)\n[PEP 492](https://www.python.org/dev/peps/pep-0492/)\n\n## Source\n- https://www.youtube.com/watch?v=iG6fr81xHKA\u0026ab_channel=PyCon2017\n- https://jimmy-huang.medium.com/python-asyncio-%E5%8D%94%E7%A8%8B-%E4%BA%8C-e717018bb984\n- https://blog.taiker.space/python-async-io-in-python-a-complete-walkthrough/\n- http://masnun.rocks/2016/10/06/async-python-the-different-forms-of-concurrency/\n","lastmodified":"2023-02-02T10:24:04.115266491Z","tags":null},"/publish/why-mypy-202301071445":{"title":"why mypy-202301071445","content":"\n# why mypy-202301071445\n\n# Summary\n\n- What is mypy\n- Python Annotations\n- Annotation PEPs\n\n# Notes\n\n## What is Mypy?\n\n[Mypy](https://github.com/python/mypy) 是一個 static type checker for Python3 \u0026 Python2.7.\n\n嗯, static type checker. 一定會有一些朋友問說為什麼 python 需要 type checker. python 不是 dynamic-typed language 嘛? 要檢查 type 就不要寫 python 了阿\n\n沒關係我們慢慢解釋\n\n## [PEP -- 483 The Theory of Type Hints](https://www.python.org/dev/peps/pep-0483/) \u0026\u0026 [PEP 484 -- Type Hints](https://www.python.org/dev/peps/pep-0484/)\n\nPEP483, PEP484 是由 Guido van Rossum, Jukka Lehtosalo, Lukasz Langa 在 2014年提出的.\n最後實現在了 python 3.5 上\n\n```python\n# 就像這樣可以指定回傳的 type -\u003e str\ndef func_a(a: str, b: str) -\u003e str:\n    return a + b\n```\n\n值得注意的一個特性是: __python 的 type hints 是 Function Annotations, function annotations 是沒有強制力的__. 如同字面上的意義它就是 function 的註解.\n\n```python\ndef foo(a: str) -\u003e str:\n    \"\"\"Foo.\"\"\"\n        return 'hello' + a\n\n# {function}.__annotations__ 可以呼叫出 Function Annotations\nprint(foo.__annotations__)\n\n# {'a': \u003cclass 'str'\u003e, 'return': \u003cclass 'str'\u003e}\n# 可以看到 `return` 內註明了我們回傳的屬性是 `str`\n```\n\n透過 Function Annotation, python 解放了一些可能性: 第三方套件可以拿到 function return type.\n\n當然在 PEP484 中也有提到 \n\n\u003e It should also be emphasized that Python will remain a dynamically typed language, and the authors have no desire to ever make type hints mandatory, even by convention.\n\n對這樣很好, 創造了很多的可能性, 但我們希望保持簡單.\n\n對這樣很好, python 永遠都是 dynamic-typed language\n\n## 更多 PEP\n\n\n### [PEP 526 -- Syntax for Variable Annotations](https://www.python.org/dev/peps/pep-0526/) \n\nPython Variable Annotations\n\n```python\ninit_var: int\n\nmy_var: int = 5\n\nother_var: int\nother_var = 5\n\nprint(__annotations__)\n\n# {'init_var': \u003cclass 'int'\u003e, 'my_var': \u003cclass 'int'\u003e, 'other_var': \u003cclass 'int'\u003e}\n```\n\n### [PEP 544 -- Protocols: Structural subtyping (static duck typing)](https://www.python.org/dev/peps/pep-0544/)\n\nPython Structural subtyping\n\n簡單理解的話, 就是 golang 的 interface.\n\n```python\nfrom typing import Iterator, Iterable\n\nclass Bucket:\n    ...\n    def __len__(self) -\u003e int: ...\n    def __iter__(self) -\u003e Iterator[int]: ...\n\ndef collect(items: Iterable[int]) -\u003e int: ...\nresult: int = collect(Bucket())  # Passes type check\n\n# Bucker 因為實作了 Iterator 的 __len__ \u0026 __iter__ . 就可以當 Iterator 使用\n```\n\n### [PEP 586 -- Literal Types](https://www.python.org/dev/peps/pep-0586/)\n\n\n在 python3.8 新增的特性: `Literal`\n\n```python\nfrom typing import Literal\n\ndef accepts_only_four(x: Literal[4]) -\u003e None:\n    pass\n\naccepts_only_four(4)   # OK\naccepts_only_four(19)  # Rejected\n```\n\n### [PEP 589 -- TypedDict: Type Hints for Dictionaries with a Fixed Set of Keys](https://www.python.org/dev/peps/pep-0589/)\n\n新增 `TypedDict`\n\n```python\nfrom typing import TypedDict\n\nclass Movie(TypedDict):\n    name: str\n    year: int\n\nmovie: Movie = {\n    'name': 'Blade Runner',\n    'year': 1982,\n}\n```\n\n### [PEP 591 -- Adding a final qualifier to typing](https://www.python.org/dev/peps/pep-0591/)\n\n新增了 `@final` \u0026\u0026 `Final`\n\n`@final` 是 decorator\n\n`Final` 是 annotation\n\n```python\nfrom typing import final\n\n@final\nclass Base:\n    ...\n\nclass Derived(Base):  # Error: Cannot inherit from final class \"Base\"\n    ...\n```\n\n```python\nfrom typing import final\n\nclass Base:\n    @final\n    def foo(self) -\u003e None:\n        ...\n\nclass Derived(Base):\n    def foo(self) -\u003e None:  # Error: Cannot override final attribute \"foo\"\n                            # (previously declared in base class \"Base\")\n        ...\n```\n\n```python\n# With an explicit type\nID: Final[float] = 1\n\n# With no type annotation.\nID: Final = 1\n```\n\n## 所以為什麼需要 Mypy?\n\n所以一切源於 PEP484, 後續的 PEP 是補充概念以及語法.\n一般來說 Mypy 可以被運用在兩個地方\n\n1. Editor check\n2. Testing\n\n我沒有辦法明確的跟你說 Mypy 是必要的且適合每個人. 比起寫一般的 python 會需要打更多字, 遵守更多規範. 但遵循這樣的規範寫程式會有幾個好處：\n\n* 易讀的程式碼\n* 易於重構的程式碼\n* 提升程式碼質量\n\nPython type hints 是屬於 annotations. 它不是強制性的規範. 但大量使用可以有效提昇程式碼的 **質量**.\n\n在日常開發以及正式的專案導入這樣的工具. 你的同事以及未來的你會更加感謝現在的你的.\n\n---\n# References\n\n[PEP 3107 -- Function Annotations](https://www.python.org/dev/peps/pep-3107/)\n","lastmodified":"2023-02-02T10:24:04.115266491Z","tags":null}}